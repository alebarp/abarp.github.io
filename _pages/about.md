---
permalink: /
title: "My background"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


I am a senior research associate at the Alan Turing Institute, working on geometric statistics and machine learning as part of [Turing-Roche partnership](https://www.turing.ac.uk/research/research-projects/alan-turing-institute-roche-strategic-partnership).

My academic journey includes completing the MMathPhys course at the University of Warwick and Part III of the Mathematical Tripos at the University of Cambridge.
As we enter the large data sets era, there is a growing necessity to comprehend the structure of data, encompassing aspects like reduction, missing data handling, and the integration of misspecification, uncertainty, and domain knowledge into our models. 
This motivated my doctoral research in statistics at Imperial College London and Postdoc at Cambridge, more specfically on Markov Chain Monte Carlo and statistical inference, which aim to sample and approximate complex distributions (see my [thesis](https://spiral.imperial.ac.uk/bitstream/10044/1/84749/1/Barp-A-A-2020-PhD-Thesis.pdf) introducing the bracket-measure formalism).
Recently, my research scope has expanded to include work on geometric deep learning, in order to gain insights in the learning process and improve our understanding of patient health heterogeneity.




The unity of statistics
======

Despite the fact geometric tools are being increasingly leveraged acrosss statistical methodologies,
geometry remains notably absent from the educational curriculum of most statistics departments,
underscoring the perception it is not directly pertinent to the training of mathematical statisticians.
It turns out that as soon as we use appropriate formalisations of probability distributions, the gap between statistics and geometry disappears. 
Find out why [here](https://drive.google.com/file/d/1OSgegqVHNjGN3XQElhmzm-D4UecBHveu/view?usp=sharing)!

The point is that the way mathematicians think of distributions has been continuously evolving. Continuous probability densities, p(x)dx, became absolutely continuous measures, normal weights, tensor 1-densities, twisted/pseudo differential forms, smooth continuous linear functionals and so on.
On this aspect I like the quote by André Lichnerowicz

<blockquote>
    <p> " [...] if we compare what was called
‘physics’ or ‘mathematics’ in the nineteenth century
with today’s physics, what would surprise us would
not be all the equations we write, but rather the
pseudo-rational entities we make up to give them
meaning. What has changed is the discourse, not
the form of the equations."
    </p>
  </blockquote>

To fully leverage the power of probability distributions, we need to stand on the giants that revolutioned mathematics and physics, which requires incorporating the unity of mathematics within our statistical methodologies.

<blockquote>
    <p> " [...] one of the most essential features of the math-
ematical world, [...] it is virtually impossible to isolate any of the above
parts from the others without depriving them from their essence. In that way the
corpus of mathematics does resemble a biological entity which can only survive as
a whole and would perish if separated into disjoint pieces." Alain Connes
    </p>
  </blockquote>
